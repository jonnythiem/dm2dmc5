{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train = pd.read_csv('C:/Users/University/Desktop/DM2/DMC_2019_task/train.csv', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totalScanned:\n",
    "train['totalScanned'] = train['scannedLineItemsPerSecond'] * train['totalScanTimeInSeconds']\n",
    "\n",
    "# avgValuePerScan:\n",
    "train['avgTimePerScan'] = 1/ train['scannedLineItemsPerSecond']\n",
    "train['avgValuePerScan'] = train['avgTimePerScan'] * train['valuePerSecond']\n",
    "\n",
    "\n",
    "\n",
    "# manual feature generation - \"totalScanned\" ratios\n",
    "\n",
    "# withoutRegisPerPosition\n",
    "train['withoutRegisPerPosition'] = train['scansWithoutRegistration'] / train['totalScanned']\n",
    "# ratio of scansWithoutRegis in totalScan\n",
    "# equivalent to lineItemVoidsPerPosition\n",
    "# Might indicate how new or ambivalent a customer is. Expected to be higher for low \"trustLevel\"\n",
    "\n",
    "# quantiModPerPosition\n",
    "train['quantiModPerPosition'] = train['quantityModifications'] / train['totalScanned']\n",
    "# ratio of quanityMods in totalScan\n",
    "\n",
    "\n",
    "\n",
    "# manual feature generation - \"grandTotal\" ratios\n",
    "\n",
    "# lineItemVoidsPerTotal\n",
    "train['lineItemVoidsPerTotal'] = train['lineItemVoids'] / train['grandTotal']\n",
    "\n",
    "# withoutRegisPerTotal\n",
    "train['withoutRegisPerTotal'] = train['scansWithoutRegistration'] / train['grandTotal']\n",
    "\n",
    "# quantiModPerTotal\n",
    "train['quantiModPerTotal'] = train['quantityModifications'] / train['grandTotal']\n",
    "\n",
    "\n",
    "\n",
    "# manual feature generation - \"totalScanTimeInSeconds\" ratios\n",
    "\n",
    "# lineItemVoidsPerTime\n",
    "train['lineItemVoidsPerTime'] = train['lineItemVoids'] / train['totalScanTimeInSeconds']\n",
    "\n",
    "# withoutRegisPerTime\n",
    "train['withoutRegisPerTime'] = train['scansWithoutRegistration'] / train['totalScanTimeInSeconds']\n",
    "\n",
    "# quantiModPerTime\n",
    "train['quantiModPerTime'] = train['quantityModifications'] / train['totalScanTimeInSeconds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems:\n",
    "# mean of fraud = 0.055 --> unbalanced data set\n",
    "# outlier handling\n",
    "# how to handle trustLevel? (unbalanced)\n",
    "\n",
    "# automated feature generation for more features\n",
    "# Subset selection with filter, wrapper or pca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = ft.EntitySet(id = 'customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = es.entity_from_dataframe(entity_id='customers',dataframe=train,\n",
    "                              variable_types = {'trustLevel': ft.variable_types.Categorical},index='id',make_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entity: customers\n",
       "  Variables:\n",
       "    id (dtype: index)\n",
       "    totalScanTimeInSeconds (dtype: numeric)\n",
       "    grandTotal (dtype: numeric)\n",
       "    lineItemVoids (dtype: numeric)\n",
       "    scansWithoutRegistration (dtype: numeric)\n",
       "    quantityModifications (dtype: numeric)\n",
       "    scannedLineItemsPerSecond (dtype: numeric)\n",
       "    valuePerSecond (dtype: numeric)\n",
       "    lineItemVoidsPerPosition (dtype: numeric)\n",
       "    fraud (dtype: numeric)\n",
       "    totalScanned (dtype: numeric)\n",
       "    avgTimePerScan (dtype: numeric)\n",
       "    avgValuePerScan (dtype: numeric)\n",
       "    withoutRegisPerPosition (dtype: numeric)\n",
       "    quantiModPerPosition (dtype: numeric)\n",
       "    lineItemVoidsPerTotal (dtype: numeric)\n",
       "    withoutRegisPerTotal (dtype: numeric)\n",
       "    quantiModPerTotal (dtype: numeric)\n",
       "    lineItemVoidsPerTime (dtype: numeric)\n",
       "    withoutRegisPerTime (dtype: numeric)\n",
       "    quantiModPerTime (dtype: numeric)\n",
       "    trustLevel (dtype: categorical)\n",
       "  Shape:\n",
       "    (Rows: 1879, Columns: 22)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es['customers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List possible aggregation primitives \n",
    "primitives = ft.list_primitives()\n",
    "pd.options.display.max_colwidth = 100\n",
    "#primitives[primitives['type'] == 'aggregation'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List possible transformation primitives\n",
    "#primitives[primitives['type'] == 'transform'].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Can take a while to compute!!!\n",
    "\n",
    "features, feature_names = ft.dfs(entityset = es, target_entity = 'customers', \n",
    "                                 agg_primitives = ['skew','trend','median', 'mean', 'max', 'std'],\n",
    "                                 trans_primitives = ['subtract', 'percentile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>fraud</th>\n",
       "      <th>totalScanned</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENTILE(withoutRegisPerPosition - lineItemVoidsPerTotal)</th>\n",
       "      <th>PERCENTILE(lineItemVoidsPerTotal - withoutRegisPerPosition)</th>\n",
       "      <th>PERCENTILE(lineItemVoidsPerPosition - valuePerSecond)</th>\n",
       "      <th>PERCENTILE(totalScanTimeInSeconds - quantiModPerTotal)</th>\n",
       "      <th>PERCENTILE(lineItemVoidsPerTime - quantiModPerTotal)</th>\n",
       "      <th>PERCENTILE(lineItemVoidsPerPosition - quantiModPerTotal)</th>\n",
       "      <th>PERCENTILE(scansWithoutRegistration - quantiModPerPosition)</th>\n",
       "      <th>PERCENTILE(scansWithoutRegistration - lineItemVoidsPerTime)</th>\n",
       "      <th>PERCENTILE(withoutRegisPerTotal - grandTotal)</th>\n",
       "      <th>PERCENTILE(withoutRegisPerTotal - valuePerSecond)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1054</td>\n",
       "      <td>54.70</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.051898</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144758</td>\n",
       "      <td>0.855774</td>\n",
       "      <td>0.405535</td>\n",
       "      <td>0.565194</td>\n",
       "      <td>0.435338</td>\n",
       "      <td>0.394891</td>\n",
       "      <td>0.068121</td>\n",
       "      <td>0.040447</td>\n",
       "      <td>0.464609</td>\n",
       "      <td>0.252262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>27.36</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220862</td>\n",
       "      <td>0.779670</td>\n",
       "      <td>0.306014</td>\n",
       "      <td>0.055349</td>\n",
       "      <td>0.223523</td>\n",
       "      <td>0.423097</td>\n",
       "      <td>0.219798</td>\n",
       "      <td>0.194252</td>\n",
       "      <td>0.731772</td>\n",
       "      <td>0.108568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1516</td>\n",
       "      <td>62.16</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.041003</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816924</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.406067</td>\n",
       "      <td>0.821181</td>\n",
       "      <td>0.276743</td>\n",
       "      <td>0.357105</td>\n",
       "      <td>0.927621</td>\n",
       "      <td>0.982970</td>\n",
       "      <td>0.384779</td>\n",
       "      <td>0.724321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1791</td>\n",
       "      <td>92.31</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016192</td>\n",
       "      <td>0.051541</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342735</td>\n",
       "      <td>0.657797</td>\n",
       "      <td>0.457158</td>\n",
       "      <td>0.970197</td>\n",
       "      <td>0.499202</td>\n",
       "      <td>0.443321</td>\n",
       "      <td>0.436402</td>\n",
       "      <td>0.431613</td>\n",
       "      <td>0.073443</td>\n",
       "      <td>0.398616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>430</td>\n",
       "      <td>81.53</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.062791</td>\n",
       "      <td>0.189605</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552954</td>\n",
       "      <td>0.447578</td>\n",
       "      <td>0.099521</td>\n",
       "      <td>0.221394</td>\n",
       "      <td>0.686535</td>\n",
       "      <td>0.276743</td>\n",
       "      <td>0.730442</td>\n",
       "      <td>0.689729</td>\n",
       "      <td>0.196381</td>\n",
       "      <td>0.158595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 801 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    totalScanTimeInSeconds  grandTotal  lineItemVoids  \\\n",
       "id                                                      \n",
       "0                     1054       54.70              7   \n",
       "1                      108       27.36              5   \n",
       "2                     1516       62.16              3   \n",
       "3                     1791       92.31              8   \n",
       "4                      430       81.53              3   \n",
       "\n",
       "    scansWithoutRegistration  quantityModifications  \\\n",
       "id                                                    \n",
       "0                          0                      3   \n",
       "1                          2                      4   \n",
       "2                         10                      5   \n",
       "3                          4                      4   \n",
       "4                          7                      2   \n",
       "\n",
       "    scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  \\\n",
       "id                                                                        \n",
       "0                    0.027514        0.051898                  0.241379   \n",
       "1                    0.129630        0.253333                  0.357143   \n",
       "2                    0.008575        0.041003                  0.230769   \n",
       "3                    0.016192        0.051541                  0.275862   \n",
       "4                    0.062791        0.189605                  0.111111   \n",
       "\n",
       "    fraud  totalScanned                        ...                          \\\n",
       "id                                             ...                           \n",
       "0       0          29.0                        ...                           \n",
       "1       0          14.0                        ...                           \n",
       "2       0          13.0                        ...                           \n",
       "3       0          29.0                        ...                           \n",
       "4       0          27.0                        ...                           \n",
       "\n",
       "    PERCENTILE(withoutRegisPerPosition - lineItemVoidsPerTotal)  \\\n",
       "id                                                                \n",
       "0                                                      0.144758   \n",
       "1                                                      0.220862   \n",
       "2                                                      0.816924   \n",
       "3                                                      0.342735   \n",
       "4                                                      0.552954   \n",
       "\n",
       "    PERCENTILE(lineItemVoidsPerTotal - withoutRegisPerPosition)  \\\n",
       "id                                                                \n",
       "0                                                      0.855774   \n",
       "1                                                      0.779670   \n",
       "2                                                      0.183608   \n",
       "3                                                      0.657797   \n",
       "4                                                      0.447578   \n",
       "\n",
       "    PERCENTILE(lineItemVoidsPerPosition - valuePerSecond)  \\\n",
       "id                                                          \n",
       "0                                                0.405535   \n",
       "1                                                0.306014   \n",
       "2                                                0.406067   \n",
       "3                                                0.457158   \n",
       "4                                                0.099521   \n",
       "\n",
       "    PERCENTILE(totalScanTimeInSeconds - quantiModPerTotal)  \\\n",
       "id                                                           \n",
       "0                                                 0.565194   \n",
       "1                                                 0.055349   \n",
       "2                                                 0.821181   \n",
       "3                                                 0.970197   \n",
       "4                                                 0.221394   \n",
       "\n",
       "    PERCENTILE(lineItemVoidsPerTime - quantiModPerTotal)  \\\n",
       "id                                                         \n",
       "0                                               0.435338   \n",
       "1                                               0.223523   \n",
       "2                                               0.276743   \n",
       "3                                               0.499202   \n",
       "4                                               0.686535   \n",
       "\n",
       "    PERCENTILE(lineItemVoidsPerPosition - quantiModPerTotal)  \\\n",
       "id                                                             \n",
       "0                                                   0.394891   \n",
       "1                                                   0.423097   \n",
       "2                                                   0.357105   \n",
       "3                                                   0.443321   \n",
       "4                                                   0.276743   \n",
       "\n",
       "    PERCENTILE(scansWithoutRegistration - quantiModPerPosition)  \\\n",
       "id                                                                \n",
       "0                                                      0.068121   \n",
       "1                                                      0.219798   \n",
       "2                                                      0.927621   \n",
       "3                                                      0.436402   \n",
       "4                                                      0.730442   \n",
       "\n",
       "    PERCENTILE(scansWithoutRegistration - lineItemVoidsPerTime)  \\\n",
       "id                                                                \n",
       "0                                                      0.040447   \n",
       "1                                                      0.194252   \n",
       "2                                                      0.982970   \n",
       "3                                                      0.431613   \n",
       "4                                                      0.689729   \n",
       "\n",
       "    PERCENTILE(withoutRegisPerTotal - grandTotal)  \\\n",
       "id                                                  \n",
       "0                                        0.464609   \n",
       "1                                        0.731772   \n",
       "2                                        0.384779   \n",
       "3                                        0.073443   \n",
       "4                                        0.196381   \n",
       "\n",
       "    PERCENTILE(withoutRegisPerTotal - valuePerSecond)  \n",
       "id                                                     \n",
       "0                                            0.252262  \n",
       "1                                            0.108568  \n",
       "2                                            0.724321  \n",
       "3                                            0.398616  \n",
       "4                                            0.158595  \n",
       "\n",
       "[5 rows x 801 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['totalScanTimeInSeconds', 'grandTotal', 'lineItemVoids',\n",
       "       'scansWithoutRegistration', 'quantityModifications',\n",
       "       'scannedLineItemsPerSecond', 'valuePerSecond',\n",
       "       'lineItemVoidsPerPosition', 'fraud', 'totalScanned',\n",
       "       ...\n",
       "       'PERCENTILE(withoutRegisPerPosition - lineItemVoidsPerTotal)',\n",
       "       'PERCENTILE(lineItemVoidsPerTotal - withoutRegisPerPosition)',\n",
       "       'PERCENTILE(lineItemVoidsPerPosition - valuePerSecond)',\n",
       "       'PERCENTILE(totalScanTimeInSeconds - quantiModPerTotal)',\n",
       "       'PERCENTILE(lineItemVoidsPerTime - quantiModPerTotal)',\n",
       "       'PERCENTILE(lineItemVoidsPerPosition - quantiModPerTotal)',\n",
       "       'PERCENTILE(scansWithoutRegistration - quantiModPerPosition)',\n",
       "       'PERCENTILE(scansWithoutRegistration - lineItemVoidsPerTime)',\n",
       "       'PERCENTILE(withoutRegisPerTotal - grandTotal)',\n",
       "       'PERCENTILE(withoutRegisPerTotal - valuePerSecond)'],\n",
       "      dtype='object', length=801)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "#from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test split\n",
    "# No Nans\n",
    "\n",
    "X = train.drop(columns='fraud', axis=1)\n",
    "y = train['fraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function of supervisor Nico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit_scorer(y, y_pred):\n",
    "    profit_matrix = {(0,0): 0, (0,1): -5, (1,0): -25, (1,1): 5}\n",
    "    return sum(profit_matrix[(pred, actual)] for pred, actual in zip(y_pred, y))\n",
    "                            # zip baut aus jedem iterierbaren object ein Tuple\n",
    "\n",
    "profit_scoring = make_scorer(profit_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model of supervisor Nico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure model without any preprocessing: \n",
      " XGB \t 520\n"
     ]
    }
   ],
   "source": [
    "# no preprocessing (no fixed seed?)\n",
    "\n",
    "df = pd.read_csv('C:/Users/University/Desktop/DM2/DMC_2019_task/train.csv', sep='|')\n",
    "X_base = features.drop(columns='fraud')\n",
    "y_base = features['fraud']\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "\n",
    "print('Pure model without any preprocessing: \\n',\n",
    "      'XGB \\t', sum(cross_validate(XGBClassifier(), X_base, y=y_base, cv=cv, scoring=profit_scoring)['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure models without any preprocessing: \n",
      " LR C2 \t 510 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# other baseline models\n",
    "print('Pure models without any preprocessing: \\n',\n",
    "      'LR C2 \\t', sum(cross_validate(LogisticRegression(C=2), X_base, y=y_base, cv=cv, scoring=profit_scoring)['test_score']), '\\n'\n",
    "                                                    #C=2 arbitrarily choosen\n",
    "#       'GNB \\t'  , sum(cross_validate(GaussianNB(), X_base, y=y_base, cv=cv, scoring=profit_scoring)['test_score']), '\\n'\n",
    "#       'DT \\t'    , sum(cross_validate(DecisionTreeClassifier(), X_base, y=y_base, cv=cv, scoring=profit_scoring)['test_score']), '\\n'\n",
    "#       'KNN \\t'    , sum(cross_validate(KNeighborsClassifier(), X_base, y=y_base, cv=cv, scoring=profit_scoring)['test_score']), '\\n'\n",
    "#       'RANFO \\t'    , sum(cross_validate(RandomForestClassifier(), X_base, y=y_base, cv=cv, scoring=profit_scoring)['test_score']), '\\n'\n",
    "#       'MLP \\t'    , sum(cross_validate(MLPClassifier(), X_base, y=y_base, cv=cv, scoring=profit_scoring)['test_score']), '\\n'\n",
    "#       'SVC \\t'    , sum(cross_validate(SVC(), X_base, y=y_base, cv=cv, scoring=profit_scoring)['test_score']), '\\n'\n",
    "     ) \n",
    "\n",
    "# Pure models without any preprocessing: \n",
    "#  LR C2 \t -295 \n",
    "# GNB \t -7185 \n",
    "# DT \t -655 \n",
    "# KNN \t -520 \n",
    "# RANFO \t -290 \n",
    "# MLP \t -815 \n",
    "# SVC \t -520 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own Model Approach with engineered features - pure (no scaling etc. implemented)\n",
    "# Commented out: brief test how much only incorporating the feature 'totalscanned' would boost our models (xgb = 205)\n",
    "\n",
    "\n",
    "# dictio = {'avgTimePerScan','avgValuePerScan','withoutRegisPerPosition','quantiModPerPosition',\n",
    "#           'lineItemVoidsPerTotal','withoutRegisPerTotal','quantiModPerTotal','lineItemVoidsPerTime','withoutRegisPerTime',\n",
    "#           'quantiModPerTime'}\n",
    "#X_wf = X.drop(columns=dictio, axis=1)\n",
    "\n",
    "\n",
    "# Init\n",
    "\n",
    "score_list_xgb = []\n",
    "score_list_lr = []\n",
    "score_list_gnb = []\n",
    "score_list_tr = []\n",
    "score_list_knn = []\n",
    "score_list_ranfo = []\n",
    "score_list_mlp = []\n",
    "score_list_svc = []\n",
    "\n",
    "\n",
    "# Cross Val Init\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "\n",
    "# Cross Val Loop\n",
    "\n",
    "for train_index, test_index in cv.split(X,y):  # adjust for X_wf\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index] # adjust for X_wf\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index] # adjust for X_wf\n",
    "    \n",
    "    # Models per split   \n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    score_xgb = profit_scorer(y_test, y_pred)\n",
    "    score_list_xgb.append(score_xgb)\n",
    "    \n",
    "    lr = LogisticRegression(C=2)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    score_lr = profit_scorer(y_test,y_pred)\n",
    "    score_list_lr.append(score_lr)\n",
    "    \n",
    "    naive_bayes = GaussianNB()\n",
    "    naive_bayes.fit(X_train, y_train)\n",
    "    y_pred = naive_bayes.predict(X_test)\n",
    "    score_gnb = profit_scorer(y_test, y_pred)\n",
    "    score_list_gnb.append(score_gnb)\n",
    "    \n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    score_tr = profit_scorer(y_test, y_pred)\n",
    "    score_list_tr.append(score_tr)  \n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    score_knn = profit_scorer(y_test, y_pred)\n",
    "    score_list_knn.append(score_knn)  \n",
    "    \n",
    "    ranfo = RandomForestClassifier()\n",
    "    ranfo.fit(X_train, y_train)\n",
    "    y_pred = ranfo.predict(X_test)\n",
    "    score_ranfo = profit_scorer(y_test, y_pred)\n",
    "    score_list_ranfo.append(score_ranfo)  \n",
    "    \n",
    "    mlp = MLPClassifier()\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    score_mlp = profit_scorer(y_test, y_pred)\n",
    "    score_list_mlp.append(score_mlp) \n",
    "    \n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    score_svc = profit_scorer(y_test, y_pred)\n",
    "    score_list_svc.append(score_svc) \n",
    "    \n",
    "    \n",
    "print('Model with engineered features (no other preprocessing:)')\n",
    "print('XGB \\t', sum(score_list_xgb))\n",
    "print('LR \\t', sum(score_list_lr))\n",
    "print('GNB \\t', sum(score_list_gnb))\n",
    "print('TR \\t', sum(score_list_tr))\n",
    "print('KNN \\t', sum(score_list_knn))\n",
    "print('RANFO \\t', sum(score_list_ranfo))\n",
    "print('MLP \\t', sum(score_list_mlp))\n",
    "print('SVC \\t', sum(score_list_svc))\n",
    "\n",
    "\n",
    "\n",
    "# Why do these tree results vary between executions of this cell(s)? We are using the same cross val seed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following you might want to commented GNB, KNN, MLP and SVC out to decrease the computing time.\n",
    "# They preform consistently the worse. Later on, when working on further hyperparameter tuning, they might be included again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Own Model Approach - feature evaluation\n",
    "# I know that its pretty quick and dirty and that we might want to use less stochastic and more sophisticated approaches\n",
    "# like PCA, etc.  \n",
    "\n",
    "\n",
    "# Init\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "dictio = {'totalScanned','avgTimePerScan','avgValuePerScan','withoutRegisPerPosition','quantiModPerPosition',\n",
    "          'lineItemVoidsPerTotal','withoutRegisPerTotal','quantiModPerTotal','lineItemVoidsPerTime','withoutRegisPerTime',\n",
    "          'quantiModPerTime'}\n",
    "\n",
    "# Eval loop\n",
    "\n",
    "for f in dictio:\n",
    "    X_wf = X\n",
    "    X_wf=X.drop(columns=f, axis=1)\n",
    "\n",
    "    # Cross Val Init\n",
    "\n",
    "    score_list_xgb = []\n",
    "    score_list_lr = []\n",
    "    score_list_gnb = []\n",
    "    score_list_tr = []\n",
    "    score_list_knn = []\n",
    "    score_list_ranfo = []\n",
    "    score_list_mlp = []\n",
    "    score_list_svc = []\n",
    "\n",
    "    # Cross Val Loop\n",
    "\n",
    "    for train_index, test_index in cv.split(X_wf,y): \n",
    "        X_train, X_test = X_wf.loc[train_index], X_wf.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "        \n",
    "        # Models per split   \n",
    "\n",
    "        xgb = XGBClassifier()\n",
    "        xgb.fit(X_train, y_train)\n",
    "        y_pred = xgb.predict(X_test)\n",
    "        score_xgb = profit_scorer(y_test, y_pred)\n",
    "        score_list_xgb.append(score_xgb)\n",
    "\n",
    "        lr = LogisticRegression(C=2)\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        score_lr = profit_scorer(y_test,y_pred)\n",
    "        score_list_lr.append(score_lr)\n",
    "\n",
    "        naive_bayes = GaussianNB()\n",
    "        naive_bayes.fit(X_train, y_train)\n",
    "        y_pred = naive_bayes.predict(X_test)\n",
    "        score_gnb = profit_scorer(y_test, y_pred)\n",
    "        score_list_gnb.append(score_gnb)\n",
    "\n",
    "        tree = DecisionTreeClassifier()\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        score_tr = profit_scorer(y_test, y_pred)\n",
    "        score_list_tr.append(score_tr)\n",
    "        \n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        score_knn = profit_scorer(y_test, y_pred)\n",
    "        score_list_knn.append(score_knn)  \n",
    "\n",
    "        ranfo = RandomForestClassifier()\n",
    "        ranfo.fit(X_train, y_train)\n",
    "        y_pred = ranfo.predict(X_test)\n",
    "        score_ranfo = profit_scorer(y_test, y_pred)\n",
    "        score_list_ranfo.append(score_ranfo)  \n",
    "\n",
    "        mlp = MLPClassifier()\n",
    "        mlp.fit(X_train, y_train)\n",
    "        y_pred = mlp.predict(X_test)\n",
    "        score_mlp = profit_scorer(y_test, y_pred)\n",
    "        score_list_mlp.append(score_mlp) \n",
    "\n",
    "        svc = SVC()\n",
    "        svc.fit(X_train, y_train)\n",
    "        y_pred = svc.predict(X_test)\n",
    "        score_svc = profit_scorer(y_test, y_pred)\n",
    "        score_list_svc.append(score_svc) \n",
    "        \n",
    "\n",
    "    print('\\n leaving out:', f)\n",
    "    print('XGB \\t', sum(score_list_xgb))\n",
    "    print('LR \\t', sum(score_list_lr))\n",
    "    print('GNB \\t', sum(score_list_gnb))\n",
    "    print('TR \\t', sum(score_list_tr))\n",
    "    print('KNN \\t', sum(score_list_knn))\n",
    "    print('RANFO \\t', sum(score_list_ranfo))\n",
    "    print('MLP \\t', sum(score_list_mlp))\n",
    "    print('SVC \\t', sum(score_list_svc))\n",
    "\n",
    "# Following regards to xgb: \n",
    "# These feature seem to have the most influence (leaving out results in a lower cost score)\n",
    "# totalScanned = 5\n",
    "# quantiModPerTime = 130\n",
    "# quantiModPerTotal = 160\n",
    "# withoutRegisPerPositio = 165\n",
    "# withoutRegisPerPositio = 165\n",
    "# withoutRegisPerTime = 170\n",
    "\n",
    "# These feature seem to have the least influence (leaving out results in a similar cost score)\n",
    "# avgTimePerScan = 230 at first glance leaving avgTimePerScan out of the model would not enhance xgb but worsen lr\n",
    "# avgValuePerScan = 215\n",
    "# lineItemVoidsPerTotal = 205\n",
    "# quantiModPerPosition = 205\n",
    "# withoutRegisPerTotal = 195\n",
    "# lineItemVoidsPerTime = 190\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Own Model Approach - only scaling\n",
    "\n",
    "\n",
    "# Following outcommented lines: I tryed to figure out whether the cost score differs, when trustlevel is not scaled\n",
    "# (It probably wont make a difference, but I guessed its more of a ordinal variable)\n",
    "# I failed because of emergence of NaNs (dont know yet where they come from)\n",
    "    \n",
    "\n",
    "# Init\n",
    "\n",
    "score_list_xgb = []\n",
    "score_list_lr = []\n",
    "score_list_gnb = []\n",
    "score_list_tr = []\n",
    "score_list_knn = []\n",
    "score_list_ranfo = []\n",
    "score_list_mlp = []\n",
    "score_list_svc = []\n",
    "\n",
    "# Cross Val Init\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "\n",
    "\n",
    "# Cross Val Loop\n",
    "\n",
    "for train_index, test_index in cv.split(X,y):   \n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "    # Scaling per split\n",
    "\n",
    "    X_train_unscal = X_train\n",
    "    #X_train_unscal = X_train.drop(['trustLevel'], axis=1)\n",
    "    scaler = prep.StandardScaler()\n",
    "    X_train_scal_np = scaler.fit_transform(X=X_train_unscal) # scaled data as np-array\n",
    "\n",
    "    X_train_cols = X_train_unscal.columns\n",
    "    X_train_scal = pd.DataFrame(X_train_scal_np, columns=X_train_cols)\n",
    "    #X_train_scal_unfinished = pd.DataFrame(X_train_scal_np, columns=X_train_cols)\n",
    "    #X_train_scal= pd.concat([X_train_scal_unfinished, X_train['trustLevel']])\n",
    "    \n",
    "    \n",
    "    X_test_unscal = X_test\n",
    "    #X_test_unscal = X_test.drop(['trustLevel'], axis=1)\n",
    "    scaler = prep.StandardScaler()\n",
    "    X_test_scal_np = scaler.fit_transform(X=X_test_unscal) # scaled data as np-array\n",
    "\n",
    "    X_test_cols = X_test_unscal.columns\n",
    "    X_test_scal = pd.DataFrame(X_test_scal_np, columns=X_test_cols)\n",
    "    #X_test_scal_unfinished = pd.DataFrame(X_test_scal_np, columns=X_train_cols)\n",
    "    #X_test_scal= pd.concat([X_test_scal_unfinished, X_test['trustLevel']])    \n",
    "    \n",
    "    \n",
    "    # Models per split   \n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_train_scal, y_train)\n",
    "    y_pred = xgb.predict(X_test_scal)\n",
    "    score_xgb = profit_scorer(y_test, y_pred)\n",
    "    score_list_xgb.append(score_xgb)\n",
    "    \n",
    "    lr = LogisticRegression(C=2)\n",
    "    lr.fit(X_train_scal, y_train)\n",
    "    y_pred = lr.predict(X_test_scal)\n",
    "    score_lr = profit_scorer(y_test,y_pred)\n",
    "    score_list_lr.append(score_lr)\n",
    "    \n",
    "    naive_bayes = GaussianNB()\n",
    "    naive_bayes.fit(X_train_scal, y_train)\n",
    "    y_pred = naive_bayes.predict(X_test_scal)\n",
    "    score_gnb = profit_scorer(y_test, y_pred)\n",
    "    score_list_gnb.append(score_gnb)\n",
    "    \n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train_scal, y_train)\n",
    "    y_pred = tree.predict(X_test_scal)\n",
    "    score_tr = profit_scorer(y_test, y_pred)\n",
    "    score_list_tr.append(score_tr)\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    score_knn = profit_scorer(y_test, y_pred)\n",
    "    score_list_knn.append(score_knn)  \n",
    "\n",
    "    ranfo = RandomForestClassifier()\n",
    "    ranfo.fit(X_train, y_train)\n",
    "    y_pred = ranfo.predict(X_test)\n",
    "    score_ranfo = profit_scorer(y_test, y_pred)\n",
    "    score_list_ranfo.append(score_ranfo)  \n",
    "\n",
    "    mlp = MLPClassifier()\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    score_mlp = profit_scorer(y_test, y_pred)\n",
    "    score_list_mlp.append(score_mlp) \n",
    "\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    score_svc = profit_scorer(y_test, y_pred)\n",
    "    score_list_svc.append(score_svc)\n",
    "\n",
    "print('Model with engineered features - scaled')\n",
    "print('XGB \\t', sum(score_list_xgb))\n",
    "print('LR \\t', sum(score_list_lr))\n",
    "print('GNB \\t', sum(score_list_gnb))\n",
    "print('TR \\t', sum(score_list_tr))\n",
    "print('KNN \\t', sum(score_list_knn))\n",
    "print('RANFO \\t', sum(score_list_ranfo))\n",
    "print('MLP \\t', sum(score_list_mlp))\n",
    "print('SVC \\t', sum(score_list_svc))\n",
    "\n",
    "# Notice LR = 230!\n",
    "# I dont really know, why only LR benefits from additional scaling. I guess the DMC send us already scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own Model Approach - upsampling (always worse off)\n",
    "\n",
    "# I know that its pretty quick and dirty and that we might want to use less stochastic approaches\n",
    "\n",
    "\n",
    "# Init\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "print('Model with engineered features - upsampled (no scaling)')\n",
    "\n",
    "# Upsampling Loop\n",
    "\n",
    "for counter_ratio in range(70,110,3):   # super dirty I know. I just wanted more refined steps than my original range(7,10)\n",
    "                                        # for loops only allow integer steps --> weird factor 10 fuckering\n",
    "    def upsam_paras(majo_len):          # upsample parameters; length of majority class\n",
    "        upsam_ratio = majo_len/(counter_ratio/10)   # '/10' would drop out if with original range(7,10). Still dirty, I know.\n",
    "        return int(upsam_ratio)\n",
    "    \n",
    "    \n",
    "    # Cross Val Init\n",
    "    \n",
    "    score_list_xgb = []\n",
    "    score_list_lr = []\n",
    "    score_list_gnb = []\n",
    "    score_list_tr = []\n",
    "    score_list_knn = []\n",
    "    score_list_ranfo = []\n",
    "    score_list_mlp = []\n",
    "    score_list_svc = []\n",
    "\n",
    "\n",
    "    # Cross Val Loop\n",
    "\n",
    "    for train_index, test_index in cv.split(X,y):\n",
    "        #X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        #y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "\n",
    "        # Upsampling per split \n",
    "\n",
    "        Xy_train = train.loc[train_index]\n",
    "        Xy_test = train.loc[test_index]\n",
    "\n",
    "        Xy_train_majo = Xy_train[Xy_train[('fraud')]==0]\n",
    "        Xy_tain_mino = Xy_train[Xy_train[('fraud')]==1]\n",
    "        Xy_train_mino_upsamp = resample(Xy_tain_mino, replace = True, n_samples=upsam_paras(len(Xy_train[Xy_train[('fraud')]==0])), random_state= 123)\n",
    "\n",
    "        Xy_train_balanced = pd.concat([Xy_train_majo, Xy_train_mino_upsamp])  \n",
    "\n",
    "        X_train = Xy_train_balanced.drop(['fraud'], axis=1)\n",
    "        y_train = Xy_train_balanced['fraud']\n",
    "\n",
    "        X_test = Xy_test.drop(['fraud'], axis=1)\n",
    "        y_test = Xy_test['fraud']\n",
    "\n",
    "\n",
    "        # Models per split   \n",
    "\n",
    "        xgb = XGBClassifier()\n",
    "        xgb.fit(X_train, y_train)\n",
    "        y_pred = xgb.predict(X_test)\n",
    "        score_xgb = profit_scorer(y_test, y_pred)\n",
    "        score_list_xgb.append(score_xgb)\n",
    "\n",
    "        lr = LogisticRegression(C=2)\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        score_lr = profit_scorer(y_test,y_pred)\n",
    "        score_list_lr.append(score_lr)\n",
    "\n",
    "        naive_bayes = GaussianNB()\n",
    "        naive_bayes.fit(X_train, y_train)\n",
    "        y_pred = naive_bayes.predict(X_test)\n",
    "        score_gnb = profit_scorer(y_test, y_pred)\n",
    "        score_list_gnb.append(score_gnb)\n",
    "\n",
    "        tree = DecisionTreeClassifier()\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        score_tr = profit_scorer(y_test, y_pred)\n",
    "        score_list_tr.append(score_tr)\n",
    "        \n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        score_knn = profit_scorer(y_test, y_pred)\n",
    "        score_list_knn.append(score_knn)  \n",
    "\n",
    "        ranfo = RandomForestClassifier()\n",
    "        ranfo.fit(X_train, y_train)\n",
    "        y_pred = ranfo.predict(X_test)\n",
    "        score_ranfo = profit_scorer(y_test, y_pred)\n",
    "        score_list_ranfo.append(score_ranfo)  \n",
    "\n",
    "        mlp = MLPClassifier()\n",
    "        mlp.fit(X_train, y_train)\n",
    "        y_pred = mlp.predict(X_test)\n",
    "        score_mlp = profit_scorer(y_test, y_pred)\n",
    "        score_list_mlp.append(score_mlp) \n",
    "\n",
    "        svc = SVC()\n",
    "        svc.fit(X_train, y_train)\n",
    "        y_pred = svc.predict(X_test)\n",
    "        score_svc = profit_scorer(y_test, y_pred)\n",
    "        score_list_svc.append(score_svc)\n",
    "\n",
    "    print('upsampled to:', upsam_paras(len(Xy_train[Xy_train[('fraud')]==0])))\n",
    "    print('XGB \\t', sum(score_list_xgb))\n",
    "    print('LR \\t', sum(score_list_lr))\n",
    "    print('GNB \\t', sum(score_list_gnb))\n",
    "    print('TR \\t', sum(score_list_tr))\n",
    "    print('KNN \\t', sum(score_list_knn))\n",
    "    print('RANFO \\t', sum(score_list_ranfo))\n",
    "    print('MLP \\t', sum(score_list_mlp))\n",
    "    print('SVC \\t', sum(score_list_svc))\n",
    "\n",
    "print('Unsampled number of minority - frauds:', len(Xy_train[Xy_train[('fraud')]==1]))\n",
    "\n",
    "# range(1,10):\n",
    "# upsampled to: 1598\n",
    "# XGB \t -165\n",
    "# upsampled to: 799\n",
    "# XGB \t -30\n",
    "# upsampled to: 532\n",
    "# XGB \t 50\n",
    "# upsampled to: 399\n",
    "# XGB \t 25\n",
    "# upsampled to: 319\n",
    "# XGB \t 15\n",
    "# upsampled to: 266\n",
    "# XGB \t 55\n",
    "# upsampled to: 228\n",
    "# XGB \t 45\n",
    "# upsampled to: 199\n",
    "# XGB \t 120             sweet spot?\n",
    "# upsampled to: 177\n",
    "# XGB \t 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own Model Approach - upsampling and scaling\n",
    "\n",
    "# I know that its pretty quick and dirty and that we might want to use less stochastic approaches\n",
    "\n",
    "\n",
    "# Init\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "print('Model with engineered features - upsampled & scaled)')\n",
    "\n",
    "# Upsampling Loop\n",
    "\n",
    "for counter_ratio in range(70,110,3):   # super dirty I know. I just wanted more refined steps than my original range(7,10)\n",
    "\n",
    "    def upsam_paras(majo_len):\n",
    "        upsam_ratio = majo_len/(counter_ratio/10)   # '/10' would drop out if with original range(7,10). Still dirty, I know.\n",
    "        return int(upsam_ratio)\n",
    "    \n",
    "    \n",
    "    # Cross Val Init\n",
    "    \n",
    "    score_list_xgb = []\n",
    "    score_list_lr = []\n",
    "    score_list_gnb = []\n",
    "    score_list_tr = []\n",
    "    score_list_knn = []\n",
    "    score_list_ranfo = []\n",
    "    score_list_mlp = []\n",
    "    score_list_svc = []\n",
    "\n",
    "\n",
    "    # Cross Val Loop\n",
    "\n",
    "    for train_index, test_index in cv.split(X,y):\n",
    "        #X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        #y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "\n",
    "        # Upsampling per split \n",
    "\n",
    "        Xy_train = train.loc[train_index]\n",
    "        Xy_test = train.loc[test_index]\n",
    "\n",
    "        Xy_train_majo = Xy_train[Xy_train[('fraud')]==0]\n",
    "        Xy_tain_mino = Xy_train[Xy_train[('fraud')]==1]\n",
    "        Xy_train_mino_upsamp = resample(Xy_tain_mino, replace = True, n_samples=upsam_paras(len(Xy_train[Xy_train[('fraud')]==0])), random_state= 123)\n",
    "\n",
    "        Xy_train_balanced = pd.concat([Xy_train_majo, Xy_train_mino_upsamp])  \n",
    "\n",
    "        X_train = Xy_train_balanced.drop(['fraud'], axis=1)\n",
    "        y_train = Xy_train_balanced['fraud']\n",
    "\n",
    "        X_test = Xy_test.drop(['fraud'], axis=1)\n",
    "        y_test = Xy_test['fraud']\n",
    "        \n",
    "            # Scaling per split\n",
    "\n",
    "        X_train_unscal = X_train\n",
    "        #X_train_unscal = X_train.drop(['trustLevel'], axis=1)\n",
    "        scaler = prep.StandardScaler()\n",
    "        X_train_scal_np = scaler.fit_transform(X=X_train_unscal) # scaled data as np-array\n",
    "\n",
    "        X_train_cols = X_train_unscal.columns\n",
    "        X_train_scal = pd.DataFrame(X_train_scal_np, columns=X_train_cols)\n",
    "        #X_train_scal_unfinished = pd.DataFrame(X_train_scal_np, columns=X_train_cols)\n",
    "        #X_train_scal= pd.concat([X_train_scal_unfinished, X_train['trustLevel']])\n",
    "\n",
    "\n",
    "        X_test_unscal = X_test\n",
    "        #X_test_unscal = X_test.drop(['trustLevel'], axis=1)\n",
    "        scaler = prep.StandardScaler()\n",
    "        X_test_scal_np = scaler.fit_transform(X=X_test_unscal) # scaled data as np-array\n",
    "\n",
    "        X_test_cols = X_test_unscal.columns\n",
    "        X_test_scal = pd.DataFrame(X_test_scal_np, columns=X_test_cols)\n",
    "        #X_test_scal_unfinished = pd.DataFrame(X_test_scal_np, columns=X_train_cols)\n",
    "        #X_test_scal= pd.concat([X_test_scal_unfinished, X_test['trustLevel']])\n",
    "\n",
    "\n",
    "\n",
    "        # Models per split   \n",
    "\n",
    "        xgb = XGBClassifier()\n",
    "        xgb.fit(X_train_scal, y_train)\n",
    "        y_pred = xgb.predict(X_test_scal)\n",
    "        score_xgb = profit_scorer(y_test, y_pred)\n",
    "        score_list_xgb.append(score_xgb)\n",
    "\n",
    "        lr = LogisticRegression(C=2)\n",
    "        lr.fit(X_train_scal, y_train)\n",
    "        y_pred = lr.predict(X_test_scal)\n",
    "        score_lr = profit_scorer(y_test,y_pred)\n",
    "        score_list_lr.append(score_lr)\n",
    "\n",
    "        naive_bayes = GaussianNB()\n",
    "        naive_bayes.fit(X_train_scal, y_train)\n",
    "        y_pred = naive_bayes.predict(X_test_scal)\n",
    "        score_gnb = profit_scorer(y_test, y_pred)\n",
    "        score_list_gnb.append(score_gnb)\n",
    "\n",
    "        tree = DecisionTreeClassifier()\n",
    "        tree.fit(X_train_scal, y_train)\n",
    "        y_pred = tree.predict(X_test_scal)\n",
    "        score_tr = profit_scorer(y_test, y_pred)\n",
    "        score_list_tr.append(score_tr)\n",
    "        \n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        score_knn = profit_scorer(y_test, y_pred)\n",
    "        score_list_knn.append(score_knn)  \n",
    "\n",
    "        ranfo = RandomForestClassifier()\n",
    "        ranfo.fit(X_train, y_train)\n",
    "        y_pred = ranfo.predict(X_test)\n",
    "        score_ranfo = profit_scorer(y_test, y_pred)\n",
    "        score_list_ranfo.append(score_ranfo)  \n",
    "\n",
    "        mlp = MLPClassifier()\n",
    "        mlp.fit(X_train, y_train)\n",
    "        y_pred = mlp.predict(X_test)\n",
    "        score_mlp = profit_scorer(y_test, y_pred)\n",
    "        score_list_mlp.append(score_mlp) \n",
    "\n",
    "        svc = SVC()\n",
    "        svc.fit(X_train, y_train)\n",
    "        y_pred = svc.predict(X_test)\n",
    "        score_svc = profit_scorer(y_test, y_pred)\n",
    "        score_list_svc.append(score_svc)\n",
    "\n",
    "\n",
    "    print('upsampled to:', upsam_paras(len(Xy_train[Xy_train[('fraud')]==0])))\n",
    "    print('XGB \\t', sum(score_list_xgb))\n",
    "    print('LR \\t', sum(score_list_lr))\n",
    "    print('GNB \\t', sum(score_list_gnb))\n",
    "    print('TR \\t', sum(score_list_tr))\n",
    "    print('KNN \\t', sum(score_list_knn))\n",
    "    print('RANFO \\t', sum(score_list_ranfo))\n",
    "    print('MLP \\t', sum(score_list_mlp))\n",
    "    print('SVC \\t', sum(score_list_svc))\n",
    "    \n",
    "print('Unsampled number of minority - frauds:', len(Xy_train[Xy_train[('fraud')]==1]))\n",
    "\n",
    "# Preformes surprisingly bad on xgb and lr. Did I do smth wrong? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model until now: XGB without scaling and no upsampling -> 230\n",
    "# and LR with only scaling -> 230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next up: XGB & LR Tuning, MetaCost Algorithm (Lecture 4, ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
