{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train = pd.read_csv('C:/Users/University/Desktop/DM2/DMC_2019_task/train.csv', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# totalScanned:\n",
    "train['totalScanned'] = train['scannedLineItemsPerSecond'] * train['totalScanTimeInSeconds']\n",
    "\n",
    "# avgValuePerScan:\n",
    "train['avgTimePerScan'] = 1/ train['scannedLineItemsPerSecond']\n",
    "train['avgValuePerScan'] = train['avgTimePerScan'] * train['valuePerSecond']\n",
    "\n",
    "\n",
    "\n",
    "# manual feature generation - \"totalScanned\" ratios\n",
    "\n",
    "# withoutRegisPerPosition\n",
    "train['withoutRegisPerPosition'] = train['scansWithoutRegistration'] / train['totalScanned']\n",
    "# ratio of scansWithoutRegis in totalScan\n",
    "# equivalent to lineItemVoidsPerPosition\n",
    "# Might indicate how new or ambivalent a customer is. Expected to be higher for low \"trustLevel\"\n",
    "\n",
    "# quantiModPerPosition\n",
    "train['quantiModPerPosition'] = train['quantityModifications'] / train['totalScanned']\n",
    "# ratio of quanityMods in totalScan\n",
    "\n",
    "\n",
    "\n",
    "# manual feature generation - \"grandTotal\" ratios\n",
    "\n",
    "# lineItemVoidsPerTotal\n",
    "train['lineItemVoidsPerTotal'] = train['lineItemVoids'] / train['grandTotal']\n",
    "\n",
    "# withoutRegisPerTotal\n",
    "train['withoutRegisPerTotal'] = train['scansWithoutRegistration'] / train['grandTotal']\n",
    "\n",
    "# quantiModPerTotal\n",
    "train['quantiModPerTotal'] = train['quantityModifications'] / train['grandTotal']\n",
    "\n",
    "\n",
    "\n",
    "# manual feature generation - \"totalScanTimeInSeconds\" ratios\n",
    "\n",
    "# lineItemVoidsPerTime\n",
    "train['lineItemVoidsPerTime'] = train['lineItemVoids'] / train['totalScanTimeInSeconds']\n",
    "\n",
    "# withoutRegisPerTime\n",
    "train['withoutRegisPerTime'] = train['scansWithoutRegistration'] / train['totalScanTimeInSeconds']\n",
    "\n",
    "# quantiModPerTime\n",
    "train['quantiModPerTime'] = train['quantityModifications'] / train['totalScanTimeInSeconds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems:\n",
    "# mean of fraud = 0.055 --> unbalanced data set\n",
    "# outlier handling\n",
    "# how to handle trustLevel? (unbalanced)\n",
    "\n",
    "# automated feature generation for more features\n",
    "# Subset selection with filter, wrapper or pca "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "#from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test split\n",
    "# No Nans\n",
    "\n",
    "X = train.drop(columns='fraud', axis=1)\n",
    "y = train['fraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function of supervisor Nico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit_scorer(y, y_pred):\n",
    "    profit_matrix = {(0,0): 0, (0,1): -5, (1,0): -25, (1,1): 5}\n",
    "    return sum(profit_matrix[(pred, actual)] for pred, actual in zip(y_pred, y))\n",
    "                            # zip baut aus jedem iterierbaren object ein Tuple\n",
    "\n",
    "profit_scoring = make_scorer(profit_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model of supervisor Nico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure model without any preprocessing: \n",
      " XGB \t -80\n"
     ]
    }
   ],
   "source": [
    "# no preprocessing (no fixed seed?)\n",
    "\n",
    "df = pd.read_csv('C:/Users/University/Desktop/DM2/DMC_2019_task/train.csv', sep='|')\n",
    "X_base = df.drop(columns='fraud')\n",
    "y_base = df['fraud']\n",
    "\n",
    "print('Pure model without any preprocessing: \\n',\n",
    "      'XGB \\t', sum(cross_validate(XGBClassifier(), X_base, y=y_base, cv=10, scoring=profit_scoring)['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure models without any preprocessing: \n",
      " LR C2 \t -295 \n",
      "GNB \t -7185 \n",
      "DT \t -655 \n",
      "KNN \t -520 \n",
      "RANFO \t -290 \n",
      "MLP \t -815 \n",
      "SVC \t -520 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# other baseline models\n",
    "print('Pure models without any preprocessing: \\n',\n",
    "      'LR C2 \\t', sum(cross_validate(LogisticRegression(C=2), X_base, y=y_base, cv=10, scoring=profit_scoring)['test_score']), '\\n'\n",
    "                                                    #C=2 arbitrarily choosen\n",
    "      'GNB \\t'  , sum(cross_validate(GaussianNB(), X_base, y=y_base, cv=10, scoring=profit_scoring)['test_score']), '\\n'\n",
    "      'DT \\t'    , sum(cross_validate(DecisionTreeClassifier(), X_base, y=y_base, cv=10, scoring=profit_scoring)['test_score']), '\\n'\n",
    "      'KNN \\t'    , sum(cross_validate(KNeighborsClassifier(), X_base, y=y_base, cv=10, scoring=profit_scoring)['test_score']), '\\n'\n",
    "      'RANFO \\t'    , sum(cross_validate(RandomForestClassifier(), X_base, y=y_base, cv=10, scoring=profit_scoring)['test_score']), '\\n'\n",
    "      'MLP \\t'    , sum(cross_validate(MLPClassifier(), X_base, y=y_base, cv=10, scoring=profit_scoring)['test_score']), '\\n'\n",
    "      'SVC \\t'    , sum(cross_validate(SVC(), X_base, y=y_base, cv=10, scoring=profit_scoring)['test_score']), '\\n'\n",
    "     ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with engineered features (no other preprocessing:)\n",
      "XGB \t 230\n",
      "LR \t 190\n",
      "GNB \t -9140\n",
      "TR \t -45\n",
      "KNN \t -910\n",
      "RANFO \t -170\n",
      "MLP \t -690\n",
      "SVC \t -520\n"
     ]
    }
   ],
   "source": [
    "# Own Model Approach with engineered features - pure (no scaling etc. implemented)\n",
    "# Commented out: brief test how much only incorporating the feature 'totalscanned' would boost our models (xgb = 205)\n",
    "\n",
    "\n",
    "# dictio = {'avgTimePerScan','avgValuePerScan','withoutRegisPerPosition','quantiModPerPosition',\n",
    "#           'lineItemVoidsPerTotal','withoutRegisPerTotal','quantiModPerTotal','lineItemVoidsPerTime','withoutRegisPerTime',\n",
    "#           'quantiModPerTime'}\n",
    "#X_wf = X.drop(columns=dictio, axis=1)\n",
    "\n",
    "\n",
    "# Init\n",
    "\n",
    "score_list_xgb = []\n",
    "score_list_lr = []\n",
    "score_list_gnb = []\n",
    "score_list_tr = []\n",
    "score_list_knn = []\n",
    "score_list_ranfo = []\n",
    "score_list_mlp = []\n",
    "score_list_svc = []\n",
    "\n",
    "\n",
    "# Cross Val Init\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "\n",
    "# Cross Val Loop\n",
    "\n",
    "for train_index, test_index in cv.split(X,y):  # adjust for X_wf\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index] # adjust for X_wf\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index] # adjust for X_wf\n",
    "    \n",
    "    # Models per split   \n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    score_xgb = profit_scorer(y_test, y_pred)\n",
    "    score_list_xgb.append(score_xgb)\n",
    "    \n",
    "    lr = LogisticRegression(C=2)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    score_lr = profit_scorer(y_test,y_pred)\n",
    "    score_list_lr.append(score_lr)\n",
    "    \n",
    "    naive_bayes = GaussianNB()\n",
    "    naive_bayes.fit(X_train, y_train)\n",
    "    y_pred = naive_bayes.predict(X_test)\n",
    "    score_gnb = profit_scorer(y_test, y_pred)\n",
    "    score_list_gnb.append(score_gnb)\n",
    "    \n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    score_tr = profit_scorer(y_test, y_pred)\n",
    "    score_list_tr.append(score_tr)  \n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    score_knn = profit_scorer(y_test, y_pred)\n",
    "    score_list_knn.append(score_knn)  \n",
    "    \n",
    "    ranfo = RandomForestClassifier()\n",
    "    ranfo.fit(X_train, y_train)\n",
    "    y_pred = ranfo.predict(X_test)\n",
    "    score_ranfo = profit_scorer(y_test, y_pred)\n",
    "    score_list_ranfo.append(score_ranfo)  \n",
    "    \n",
    "    mlp = MLPClassifier()\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    score_mlp = profit_scorer(y_test, y_pred)\n",
    "    score_list_mlp.append(score_mlp) \n",
    "    \n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    score_svc = profit_scorer(y_test, y_pred)\n",
    "    score_list_svc.append(score_svc) \n",
    "    \n",
    "    \n",
    "print('Model with engineered features (no other preprocessing:)')\n",
    "print('XGB \\t', sum(score_list_xgb))\n",
    "print('LR \\t', sum(score_list_lr))\n",
    "print('GNB \\t', sum(score_list_gnb))\n",
    "print('TR \\t', sum(score_list_tr))\n",
    "print('KNN \\t', sum(score_list_knn))\n",
    "print('RANFO \\t', sum(score_list_ranfo))\n",
    "print('MLP \\t', sum(score_list_mlp))\n",
    "print('SVC \\t', sum(score_list_svc))\n",
    "\n",
    "\n",
    "\n",
    "# Why do these tree results vary between executions of this cell(s)? We are using the same cross val seed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following you might want to commented GNB, KNN, MLP and SVC out to decrease the computing time.\n",
    "# They preform consistently the worse. Later on, when working on further hyperparameter tuning, they might be included again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " leaving out: avgTimePerScan\n",
      "XGB \t 230\n",
      "LR \t 70\n",
      "GNB \t -9190\n",
      "TR \t -165\n",
      "KNN \t -620\n",
      "RANFO \t 25\n",
      "MLP \t -1015\n",
      "SVC \t -520\n",
      "\n",
      " leaving out: lineItemVoidsPerTotal\n",
      "XGB \t 205\n",
      "LR \t 225\n",
      "GNB \t -8740\n",
      "TR \t -200\n",
      "KNN \t -910\n",
      "RANFO \t 40\n",
      "MLP \t -495\n",
      "SVC \t -520\n",
      "\n",
      " leaving out: avgValuePerScan\n",
      "XGB \t 215\n",
      "LR \t 190\n",
      "GNB \t -9165\n",
      "TR \t -220\n",
      "KNN \t -910\n",
      "RANFO \t 0\n",
      "MLP \t -605\n",
      "SVC \t -520\n",
      "\n",
      " leaving out: quantiModPerTime\n",
      "XGB \t 130\n",
      "LR \t 200\n",
      "GNB \t -8865\n",
      "TR \t -45\n",
      "KNN \t -910\n",
      "RANFO \t -15\n",
      "MLP \t -725\n",
      "SVC \t -520\n",
      "\n",
      " leaving out: lineItemVoidsPerTime\n",
      "XGB \t 190\n",
      "LR \t 190\n",
      "GNB \t -8765\n",
      "TR \t -215\n",
      "KNN \t -910\n",
      "RANFO \t -100\n",
      "MLP \t -945\n",
      "SVC \t -520\n",
      "\n",
      " leaving out: withoutRegisPerPosition\n",
      "XGB \t 165\n",
      "LR \t 200\n",
      "GNB \t -8915\n",
      "TR \t -240\n",
      "KNN \t -910\n",
      "RANFO \t -60\n",
      "MLP \t -1200\n",
      "SVC \t -520\n",
      "\n",
      " leaving out: quantiModPerTotal\n",
      "XGB \t 160\n",
      "LR \t 200\n",
      "GNB \t -9115\n",
      "TR \t -175\n",
      "KNN \t -910\n",
      "RANFO \t -85\n",
      "MLP \t -735\n",
      "SVC \t -520\n",
      "\n",
      " leaving out: quantiModPerPosition\n",
      "XGB \t 205\n",
      "LR \t 190\n",
      "GNB \t -9090\n",
      "TR \t -195\n",
      "KNN \t -910\n",
      "RANFO \t 30\n",
      "MLP \t -495\n",
      "SVC \t -520\n",
      "\n",
      " leaving out: withoutRegisPerTime\n",
      "XGB \t 170\n",
      "LR \t 165\n",
      "GNB \t -8815\n",
      "TR \t -155\n",
      "KNN \t -910\n",
      "RANFO \t 20\n",
      "MLP \t -905\n",
      "SVC \t -520\n",
      "\n",
      " leaving out: totalScanned\n",
      "XGB \t 5\n",
      "LR \t 175\n",
      "GNB \t -10840\n",
      "TR \t -825\n",
      "KNN \t -885\n",
      "RANFO \t -280\n",
      "MLP \t -245\n",
      "SVC \t -520\n",
      "\n",
      " leaving out: withoutRegisPerTotal\n",
      "XGB \t 195\n",
      "LR \t 190\n",
      "GNB \t -8740\n",
      "TR \t -245\n",
      "KNN \t -910\n",
      "RANFO \t -20\n",
      "MLP \t -545\n",
      "SVC \t -520\n"
     ]
    }
   ],
   "source": [
    "# Own Model Approach - feature evaluation\n",
    "# I know that its pretty quick and dirty and that we might want to use less stochastic and more sophisticated approaches\n",
    "# like PCA, etc.  \n",
    "\n",
    "\n",
    "# Init\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "dictio = {'totalScanned','avgTimePerScan','avgValuePerScan','withoutRegisPerPosition','quantiModPerPosition',\n",
    "          'lineItemVoidsPerTotal','withoutRegisPerTotal','quantiModPerTotal','lineItemVoidsPerTime','withoutRegisPerTime',\n",
    "          'quantiModPerTime'}\n",
    "\n",
    "# Eval loop\n",
    "\n",
    "for f in dictio:\n",
    "    X_wf = X\n",
    "    X_wf=X.drop(columns=f, axis=1)\n",
    "\n",
    "    # Cross Val Init\n",
    "\n",
    "    score_list_xgb = []\n",
    "    score_list_lr = []\n",
    "    score_list_gnb = []\n",
    "    score_list_tr = []\n",
    "    score_list_knn = []\n",
    "    score_list_ranfo = []\n",
    "    score_list_mlp = []\n",
    "    score_list_svc = []\n",
    "\n",
    "    # Cross Val Loop\n",
    "\n",
    "    for train_index, test_index in cv.split(X_wf,y): \n",
    "        X_train, X_test = X_wf.loc[train_index], X_wf.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "        \n",
    "        # Models per split   \n",
    "\n",
    "        xgb = XGBClassifier()\n",
    "        xgb.fit(X_train, y_train)\n",
    "        y_pred = xgb.predict(X_test)\n",
    "        score_xgb = profit_scorer(y_test, y_pred)\n",
    "        score_list_xgb.append(score_xgb)\n",
    "\n",
    "        lr = LogisticRegression(C=2)\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        score_lr = profit_scorer(y_test,y_pred)\n",
    "        score_list_lr.append(score_lr)\n",
    "\n",
    "        naive_bayes = GaussianNB()\n",
    "        naive_bayes.fit(X_train, y_train)\n",
    "        y_pred = naive_bayes.predict(X_test)\n",
    "        score_gnb = profit_scorer(y_test, y_pred)\n",
    "        score_list_gnb.append(score_gnb)\n",
    "\n",
    "        tree = DecisionTreeClassifier()\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        score_tr = profit_scorer(y_test, y_pred)\n",
    "        score_list_tr.append(score_tr)\n",
    "        \n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        score_knn = profit_scorer(y_test, y_pred)\n",
    "        score_list_knn.append(score_knn)  \n",
    "\n",
    "        ranfo = RandomForestClassifier()\n",
    "        ranfo.fit(X_train, y_train)\n",
    "        y_pred = ranfo.predict(X_test)\n",
    "        score_ranfo = profit_scorer(y_test, y_pred)\n",
    "        score_list_ranfo.append(score_ranfo)  \n",
    "\n",
    "        mlp = MLPClassifier()\n",
    "        mlp.fit(X_train, y_train)\n",
    "        y_pred = mlp.predict(X_test)\n",
    "        score_mlp = profit_scorer(y_test, y_pred)\n",
    "        score_list_mlp.append(score_mlp) \n",
    "\n",
    "        svc = SVC()\n",
    "        svc.fit(X_train, y_train)\n",
    "        y_pred = svc.predict(X_test)\n",
    "        score_svc = profit_scorer(y_test, y_pred)\n",
    "        score_list_svc.append(score_svc) \n",
    "        \n",
    "\n",
    "    print('\\n leaving out:', f)\n",
    "    print('XGB \\t', sum(score_list_xgb))\n",
    "    print('LR \\t', sum(score_list_lr))\n",
    "    print('GNB \\t', sum(score_list_gnb))\n",
    "    print('TR \\t', sum(score_list_tr))\n",
    "    print('KNN \\t', sum(score_list_knn))\n",
    "    print('RANFO \\t', sum(score_list_ranfo))\n",
    "    print('MLP \\t', sum(score_list_mlp))\n",
    "    print('SVC \\t', sum(score_list_svc))\n",
    "\n",
    "# Following regards to xgb: \n",
    "# These feature seem to have the most influence (leaving out results in a lower cost score)\n",
    "# totalScanned = 5\n",
    "# quantiModPerTime = 130\n",
    "# quantiModPerTotal = 160\n",
    "# withoutRegisPerPositio = 165\n",
    "# withoutRegisPerPositio = 165\n",
    "# withoutRegisPerTime = 170\n",
    "\n",
    "# These feature seem to have the least influence (leaving out results in a similar cost score)\n",
    "# avgTimePerScan = 230 at first glance leaving avgTimePerScan out of the model would not enhance xgb but worsen lr\n",
    "# avgValuePerScan = 215\n",
    "# lineItemVoidsPerTotal = 205\n",
    "# quantiModPerPosition = 205\n",
    "# withoutRegisPerTotal = 195\n",
    "# lineItemVoidsPerTime = 190\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with engineered features - scaled\n",
      "XGB \t -15\n",
      "LR \t 230\n",
      "GNB \t -3755\n",
      "TR \t -535\n",
      "KNN \t -910\n",
      "RANFO \t -120\n",
      "MLP \t -735\n",
      "SVC \t -520\n"
     ]
    }
   ],
   "source": [
    "# Own Model Approach - only scaling\n",
    "\n",
    "\n",
    "# Following outcommented lines: I tryed to figure out whether the cost score differs, when trustlevel is not scaled\n",
    "# (It probably wont make a difference, but I guessed its more of a ordinal variable)\n",
    "# I failed because of emergence of NaNs (dont know yet where they come from)\n",
    "    \n",
    "\n",
    "# Init\n",
    "\n",
    "score_list_xgb = []\n",
    "score_list_lr = []\n",
    "score_list_gnb = []\n",
    "score_list_tr = []\n",
    "score_list_knn = []\n",
    "score_list_ranfo = []\n",
    "score_list_mlp = []\n",
    "score_list_svc = []\n",
    "\n",
    "# Cross Val Init\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "\n",
    "\n",
    "# Cross Val Loop\n",
    "\n",
    "for train_index, test_index in cv.split(X,y):   \n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "    # Scaling per split\n",
    "\n",
    "    X_train_unscal = X_train\n",
    "    #X_train_unscal = X_train.drop(['trustLevel'], axis=1)\n",
    "    scaler = prep.StandardScaler()\n",
    "    X_train_scal_np = scaler.fit_transform(X=X_train_unscal) # scaled data as np-array\n",
    "\n",
    "    X_train_cols = X_train_unscal.columns\n",
    "    X_train_scal = pd.DataFrame(X_train_scal_np, columns=X_train_cols)\n",
    "    #X_train_scal_unfinished = pd.DataFrame(X_train_scal_np, columns=X_train_cols)\n",
    "    #X_train_scal= pd.concat([X_train_scal_unfinished, X_train['trustLevel']])\n",
    "    \n",
    "    \n",
    "    X_test_unscal = X_test\n",
    "    #X_test_unscal = X_test.drop(['trustLevel'], axis=1)\n",
    "    scaler = prep.StandardScaler()\n",
    "    X_test_scal_np = scaler.fit_transform(X=X_test_unscal) # scaled data as np-array\n",
    "\n",
    "    X_test_cols = X_test_unscal.columns\n",
    "    X_test_scal = pd.DataFrame(X_test_scal_np, columns=X_test_cols)\n",
    "    #X_test_scal_unfinished = pd.DataFrame(X_test_scal_np, columns=X_train_cols)\n",
    "    #X_test_scal= pd.concat([X_test_scal_unfinished, X_test['trustLevel']])    \n",
    "    \n",
    "    \n",
    "    # Models per split   \n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_train_scal, y_train)\n",
    "    y_pred = xgb.predict(X_test_scal)\n",
    "    score_xgb = profit_scorer(y_test, y_pred)\n",
    "    score_list_xgb.append(score_xgb)\n",
    "    \n",
    "    lr = LogisticRegression(C=2)\n",
    "    lr.fit(X_train_scal, y_train)\n",
    "    y_pred = lr.predict(X_test_scal)\n",
    "    score_lr = profit_scorer(y_test,y_pred)\n",
    "    score_list_lr.append(score_lr)\n",
    "    \n",
    "    naive_bayes = GaussianNB()\n",
    "    naive_bayes.fit(X_train_scal, y_train)\n",
    "    y_pred = naive_bayes.predict(X_test_scal)\n",
    "    score_gnb = profit_scorer(y_test, y_pred)\n",
    "    score_list_gnb.append(score_gnb)\n",
    "    \n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train_scal, y_train)\n",
    "    y_pred = tree.predict(X_test_scal)\n",
    "    score_tr = profit_scorer(y_test, y_pred)\n",
    "    score_list_tr.append(score_tr)\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    score_knn = profit_scorer(y_test, y_pred)\n",
    "    score_list_knn.append(score_knn)  \n",
    "\n",
    "    ranfo = RandomForestClassifier()\n",
    "    ranfo.fit(X_train, y_train)\n",
    "    y_pred = ranfo.predict(X_test)\n",
    "    score_ranfo = profit_scorer(y_test, y_pred)\n",
    "    score_list_ranfo.append(score_ranfo)  \n",
    "\n",
    "    mlp = MLPClassifier()\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    score_mlp = profit_scorer(y_test, y_pred)\n",
    "    score_list_mlp.append(score_mlp) \n",
    "\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    score_svc = profit_scorer(y_test, y_pred)\n",
    "    score_list_svc.append(score_svc)\n",
    "\n",
    "print('Model with engineered features - scaled')\n",
    "print('XGB \\t', sum(score_list_xgb))\n",
    "print('LR \\t', sum(score_list_lr))\n",
    "print('GNB \\t', sum(score_list_gnb))\n",
    "print('TR \\t', sum(score_list_tr))\n",
    "print('KNN \\t', sum(score_list_knn))\n",
    "print('RANFO \\t', sum(score_list_ranfo))\n",
    "print('MLP \\t', sum(score_list_mlp))\n",
    "print('SVC \\t', sum(score_list_svc))\n",
    "\n",
    "# Notice LR = 230!\n",
    "# I dont really know, why only LR benefits from additional scaling. I guess the DMC send us already scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with engineered features - upsampled (no scaling)\n",
      "upsampled to: 228\n",
      "XGB \t 45\n",
      "LR \t 60\n",
      "GNB \t -9690\n",
      "TR \t -345\n",
      "KNN \t -3435\n",
      "RANFO \t -85\n",
      "MLP \t -1540\n",
      "SVC \t -520\n",
      "upsampled to: 218\n",
      "XGB \t 95\n",
      "LR \t 60\n",
      "GNB \t -9690\n",
      "TR \t -285\n",
      "KNN \t -3395\n",
      "RANFO \t -40\n",
      "MLP \t -1590\n",
      "SVC \t -520\n",
      "upsampled to: 210\n",
      "XGB \t 95\n",
      "LR \t 75\n",
      "GNB \t -9790\n",
      "TR \t -285\n",
      "KNN \t -3135\n",
      "RANFO \t 60\n",
      "MLP \t -830\n",
      "SVC \t -520\n",
      "upsampled to: 202\n",
      "XGB \t 80\n",
      "LR \t 75\n",
      "GNB \t -9765\n",
      "TR \t -330\n",
      "KNN \t -3120\n",
      "RANFO \t -35\n",
      "MLP \t -465\n",
      "SVC \t -520\n",
      "upsampled to: 194\n",
      "XGB \t 120\n",
      "LR \t 100\n",
      "GNB \t -9640\n",
      "TR \t -315\n",
      "KNN \t -3120\n",
      "RANFO \t -35\n",
      "MLP \t -830\n",
      "SVC \t -520\n",
      "upsampled to: 188\n",
      "XGB \t 95\n",
      "LR \t 100\n",
      "GNB \t -9565\n",
      "TR \t -440\n",
      "KNN \t -2945\n",
      "RANFO \t -80\n",
      "MLP \t -805\n",
      "SVC \t -520\n",
      "upsampled to: 181\n",
      "XGB \t 20\n",
      "LR \t 90\n",
      "GNB \t -9515\n",
      "TR \t -385\n",
      "KNN \t -2870\n",
      "RANFO \t -40\n",
      "MLP \t -530\n",
      "SVC \t -520\n",
      "upsampled to: 175\n",
      "XGB \t 20\n",
      "LR \t 90\n",
      "GNB \t -9515\n",
      "TR \t -390\n",
      "KNN \t -2780\n",
      "RANFO \t -205\n",
      "MLP \t -485\n",
      "SVC \t -520\n",
      "upsampled to: 170\n",
      "XGB \t 5\n",
      "LR \t 90\n",
      "GNB \t -9490\n",
      "TR \t -360\n",
      "KNN \t -2655\n",
      "RANFO \t -105\n",
      "MLP \t -210\n",
      "SVC \t -520\n",
      "upsampled to: 164\n",
      "XGB \t 80\n",
      "LR \t 90\n",
      "GNB \t -9490\n",
      "TR \t -310\n",
      "KNN \t -2580\n",
      "RANFO \t -140\n",
      "MLP \t -295\n",
      "SVC \t -520\n",
      "upsampled to: 159\n",
      "XGB \t 0\n",
      "LR \t 80\n",
      "GNB \t -9465\n",
      "TR \t -425\n",
      "KNN \t -2475\n",
      "RANFO \t -85\n",
      "MLP \t -305\n",
      "SVC \t -520\n",
      "upsampled to: 155\n",
      "XGB \t 65\n",
      "LR \t 90\n",
      "GNB \t -9490\n",
      "TR \t -390\n",
      "KNN \t -2350\n",
      "RANFO \t -100\n",
      "MLP \t -595\n",
      "SVC \t -520\n",
      "upsampled to: 150\n",
      "XGB \t 10\n",
      "LR \t 105\n",
      "GNB \t -9315\n",
      "TR \t -350\n",
      "KNN \t -2260\n",
      "RANFO \t -140\n",
      "MLP \t -45\n",
      "SVC \t -520\n",
      "upsampled to: 146\n",
      "XGB \t 70\n",
      "LR \t 115\n",
      "GNB \t -9440\n",
      "TR \t -340\n",
      "KNN \t -2270\n",
      "RANFO \t -70\n",
      "MLP \t -550\n",
      "SVC \t -520\n",
      "Unsampled number of minority - frauds: 94\n"
     ]
    }
   ],
   "source": [
    "# Own Model Approach - upsampling (always worse off)\n",
    "\n",
    "# I know that its pretty quick and dirty and that we might want to use less stochastic approaches\n",
    "\n",
    "\n",
    "# Init\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "print('Model with engineered features - upsampled (no scaling)')\n",
    "\n",
    "# Upsampling Loop\n",
    "\n",
    "for counter_ratio in range(70,110,3):   # super dirty I know. I just wanted more refined steps than my original range(7,10)\n",
    "                                        # for loops only allow integer steps --> weird factor 10 fuckering\n",
    "    def upsam_paras(majo_len):          # upsample parameters; length of majority class\n",
    "        upsam_ratio = majo_len/(counter_ratio/10)   # '/10' would drop out if with original range(7,10). Still dirty, I know.\n",
    "        return int(upsam_ratio)\n",
    "    \n",
    "    \n",
    "    # Cross Val Init\n",
    "    \n",
    "    score_list_xgb = []\n",
    "    score_list_lr = []\n",
    "    score_list_gnb = []\n",
    "    score_list_tr = []\n",
    "    score_list_knn = []\n",
    "    score_list_ranfo = []\n",
    "    score_list_mlp = []\n",
    "    score_list_svc = []\n",
    "\n",
    "\n",
    "    # Cross Val Loop\n",
    "\n",
    "    for train_index, test_index in cv.split(X,y):\n",
    "        #X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        #y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "\n",
    "        # Upsampling per split \n",
    "\n",
    "        Xy_train = train.loc[train_index]\n",
    "        Xy_test = train.loc[test_index]\n",
    "\n",
    "        Xy_train_majo = Xy_train[Xy_train[('fraud')]==0]\n",
    "        Xy_tain_mino = Xy_train[Xy_train[('fraud')]==1]\n",
    "        Xy_train_mino_upsamp = resample(Xy_tain_mino, replace = True, n_samples=upsam_paras(len(Xy_train[Xy_train[('fraud')]==0])), random_state= 123)\n",
    "\n",
    "        Xy_train_balanced = pd.concat([Xy_train_majo, Xy_train_mino_upsamp])  \n",
    "\n",
    "        X_train = Xy_train_balanced.drop(['fraud'], axis=1)\n",
    "        y_train = Xy_train_balanced['fraud']\n",
    "\n",
    "        X_test = Xy_test.drop(['fraud'], axis=1)\n",
    "        y_test = Xy_test['fraud']\n",
    "\n",
    "\n",
    "        # Models per split   \n",
    "\n",
    "        xgb = XGBClassifier()\n",
    "        xgb.fit(X_train, y_train)\n",
    "        y_pred = xgb.predict(X_test)\n",
    "        score_xgb = profit_scorer(y_test, y_pred)\n",
    "        score_list_xgb.append(score_xgb)\n",
    "\n",
    "        lr = LogisticRegression(C=2)\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        score_lr = profit_scorer(y_test,y_pred)\n",
    "        score_list_lr.append(score_lr)\n",
    "\n",
    "        naive_bayes = GaussianNB()\n",
    "        naive_bayes.fit(X_train, y_train)\n",
    "        y_pred = naive_bayes.predict(X_test)\n",
    "        score_gnb = profit_scorer(y_test, y_pred)\n",
    "        score_list_gnb.append(score_gnb)\n",
    "\n",
    "        tree = DecisionTreeClassifier()\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_test)\n",
    "        score_tr = profit_scorer(y_test, y_pred)\n",
    "        score_list_tr.append(score_tr)\n",
    "        \n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        score_knn = profit_scorer(y_test, y_pred)\n",
    "        score_list_knn.append(score_knn)  \n",
    "\n",
    "        ranfo = RandomForestClassifier()\n",
    "        ranfo.fit(X_train, y_train)\n",
    "        y_pred = ranfo.predict(X_test)\n",
    "        score_ranfo = profit_scorer(y_test, y_pred)\n",
    "        score_list_ranfo.append(score_ranfo)  \n",
    "\n",
    "        mlp = MLPClassifier()\n",
    "        mlp.fit(X_train, y_train)\n",
    "        y_pred = mlp.predict(X_test)\n",
    "        score_mlp = profit_scorer(y_test, y_pred)\n",
    "        score_list_mlp.append(score_mlp) \n",
    "\n",
    "        svc = SVC()\n",
    "        svc.fit(X_train, y_train)\n",
    "        y_pred = svc.predict(X_test)\n",
    "        score_svc = profit_scorer(y_test, y_pred)\n",
    "        score_list_svc.append(score_svc)\n",
    "\n",
    "    print('upsampled to:', upsam_paras(len(Xy_train[Xy_train[('fraud')]==0])))\n",
    "    print('XGB \\t', sum(score_list_xgb))\n",
    "    print('LR \\t', sum(score_list_lr))\n",
    "    print('GNB \\t', sum(score_list_gnb))\n",
    "    print('TR \\t', sum(score_list_tr))\n",
    "    print('KNN \\t', sum(score_list_knn))\n",
    "    print('RANFO \\t', sum(score_list_ranfo))\n",
    "    print('MLP \\t', sum(score_list_mlp))\n",
    "    print('SVC \\t', sum(score_list_svc))\n",
    "\n",
    "print('Unsampled number of minority - frauds:', len(Xy_train[Xy_train[('fraud')]==1]))\n",
    "\n",
    "# range(1,10):\n",
    "# upsampled to: 1598\n",
    "# XGB \t -165\n",
    "# upsampled to: 799\n",
    "# XGB \t -30\n",
    "# upsampled to: 532\n",
    "# XGB \t 50\n",
    "# upsampled to: 399\n",
    "# XGB \t 25\n",
    "# upsampled to: 319\n",
    "# XGB \t 15\n",
    "# upsampled to: 266\n",
    "# XGB \t 55\n",
    "# upsampled to: 228\n",
    "# XGB \t 45\n",
    "# upsampled to: 199\n",
    "# XGB \t 120             sweet spot?\n",
    "# upsampled to: 177\n",
    "# XGB \t 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with engineered features - upsampled & scaled)\n",
      "upsampled to: 228\n",
      "XGB \t 35\n",
      "LR \t -1165\n",
      "GNB \t -4815\n",
      "TR \t -405\n",
      "KNN \t -3435\n",
      "RANFO \t -45\n",
      "MLP \t -595\n",
      "SVC \t -520\n",
      "upsampled to: 218\n",
      "XGB \t 15\n",
      "LR \t -1040\n",
      "GNB \t -4780\n",
      "TR \t -665\n",
      "KNN \t -3395\n",
      "RANFO \t -75\n",
      "MLP \t -390\n",
      "SVC \t -520\n",
      "upsampled to: 210\n",
      "XGB \t 25\n",
      "LR \t -965\n",
      "GNB \t -4780\n",
      "TR \t -495\n",
      "KNN \t -3135\n",
      "RANFO \t -160\n",
      "MLP \t -1005\n",
      "SVC \t -520\n",
      "upsampled to: 202\n",
      "XGB \t -35\n",
      "LR \t -750\n",
      "GNB \t -4755\n",
      "TR \t -485\n",
      "KNN \t -3120\n",
      "RANFO \t -80\n",
      "MLP \t -815\n",
      "SVC \t -520\n",
      "upsampled to: 194\n",
      "XGB \t -45\n",
      "LR \t -760\n",
      "GNB \t -4690\n",
      "TR \t -565\n",
      "KNN \t -3120\n",
      "RANFO \t -20\n",
      "MLP \t -405\n",
      "SVC \t -520\n",
      "upsampled to: 188\n",
      "XGB \t -20\n",
      "LR \t -660\n",
      "GNB \t -4665\n",
      "TR \t -360\n",
      "KNN \t -2945\n",
      "RANFO \t -20\n",
      "MLP \t -720\n",
      "SVC \t -520\n",
      "upsampled to: 181\n",
      "XGB \t -15\n",
      "LR \t -635\n",
      "GNB \t -4715\n",
      "TR \t -405\n",
      "KNN \t -2870\n",
      "RANFO \t -25\n",
      "MLP \t -690\n",
      "SVC \t -520\n",
      "upsampled to: 175\n",
      "XGB \t 10\n",
      "LR \t -605\n",
      "GNB \t -4640\n",
      "TR \t -380\n",
      "KNN \t -2780\n",
      "RANFO \t -145\n",
      "MLP \t -250\n",
      "SVC \t -520\n",
      "upsampled to: 170\n",
      "XGB \t 10\n",
      "LR \t -505\n",
      "GNB \t -4665\n",
      "TR \t -215\n",
      "KNN \t -2655\n",
      "RANFO \t -105\n",
      "MLP \t -650\n",
      "SVC \t -520\n",
      "upsampled to: 164\n",
      "XGB \t 55\n",
      "LR \t -480\n",
      "GNB \t -4640\n",
      "TR \t -190\n",
      "KNN \t -2580\n",
      "RANFO \t -110\n",
      "MLP \t -640\n",
      "SVC \t -520\n",
      "upsampled to: 159\n",
      "XGB \t 25\n",
      "LR \t -465\n",
      "GNB \t -4265\n",
      "TR \t -425\n",
      "KNN \t -2475\n",
      "RANFO \t -10\n",
      "MLP \t -385\n",
      "SVC \t -520\n",
      "upsampled to: 155\n",
      "XGB \t 10\n",
      "LR \t -440\n",
      "GNB \t -4190\n",
      "TR \t -330\n",
      "KNN \t -2350\n",
      "RANFO \t -130\n",
      "MLP \t -995\n",
      "SVC \t -520\n",
      "upsampled to: 150\n",
      "XGB \t -10\n",
      "LR \t -440\n",
      "GNB \t -4225\n",
      "TR \t -365\n",
      "KNN \t -2260\n",
      "RANFO \t -105\n",
      "MLP \t -555\n",
      "SVC \t -520\n",
      "upsampled to: 146\n",
      "XGB \t -25\n",
      "LR \t -390\n",
      "GNB \t -3665\n",
      "TR \t -275\n",
      "KNN \t -2270\n",
      "RANFO \t -25\n",
      "MLP \t -305\n",
      "SVC \t -520\n",
      "Unsampled number of minority - frauds: 94\n"
     ]
    }
   ],
   "source": [
    "# Own Model Approach - upsampling and scaling\n",
    "\n",
    "# I know that its pretty quick and dirty and that we might want to use less stochastic approaches\n",
    "\n",
    "\n",
    "# Init\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "print('Model with engineered features - upsampled & scaled)')\n",
    "\n",
    "# Upsampling Loop\n",
    "\n",
    "for counter_ratio in range(70,110,3):   # super dirty I know. I just wanted more refined steps than my original range(7,10)\n",
    "\n",
    "    def upsam_paras(majo_len):\n",
    "        upsam_ratio = majo_len/(counter_ratio/10)   # '/10' would drop out if with original range(7,10). Still dirty, I know.\n",
    "        return int(upsam_ratio)\n",
    "    \n",
    "    \n",
    "    # Cross Val Init\n",
    "    \n",
    "    score_list_xgb = []\n",
    "    score_list_lr = []\n",
    "    score_list_gnb = []\n",
    "    score_list_tr = []\n",
    "    score_list_knn = []\n",
    "    score_list_ranfo = []\n",
    "    score_list_mlp = []\n",
    "    score_list_svc = []\n",
    "\n",
    "\n",
    "    # Cross Val Loop\n",
    "\n",
    "    for train_index, test_index in cv.split(X,y):\n",
    "        #X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        #y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "\n",
    "        # Upsampling per split \n",
    "\n",
    "        Xy_train = train.loc[train_index]\n",
    "        Xy_test = train.loc[test_index]\n",
    "\n",
    "        Xy_train_majo = Xy_train[Xy_train[('fraud')]==0]\n",
    "        Xy_tain_mino = Xy_train[Xy_train[('fraud')]==1]\n",
    "        Xy_train_mino_upsamp = resample(Xy_tain_mino, replace = True, n_samples=upsam_paras(len(Xy_train[Xy_train[('fraud')]==0])), random_state= 123)\n",
    "\n",
    "        Xy_train_balanced = pd.concat([Xy_train_majo, Xy_train_mino_upsamp])  \n",
    "\n",
    "        X_train = Xy_train_balanced.drop(['fraud'], axis=1)\n",
    "        y_train = Xy_train_balanced['fraud']\n",
    "\n",
    "        X_test = Xy_test.drop(['fraud'], axis=1)\n",
    "        y_test = Xy_test['fraud']\n",
    "        \n",
    "            # Scaling per split\n",
    "\n",
    "        X_train_unscal = X_train\n",
    "        #X_train_unscal = X_train.drop(['trustLevel'], axis=1)\n",
    "        scaler = prep.StandardScaler()\n",
    "        X_train_scal_np = scaler.fit_transform(X=X_train_unscal) # scaled data as np-array\n",
    "\n",
    "        X_train_cols = X_train_unscal.columns\n",
    "        X_train_scal = pd.DataFrame(X_train_scal_np, columns=X_train_cols)\n",
    "        #X_train_scal_unfinished = pd.DataFrame(X_train_scal_np, columns=X_train_cols)\n",
    "        #X_train_scal= pd.concat([X_train_scal_unfinished, X_train['trustLevel']])\n",
    "\n",
    "\n",
    "        X_test_unscal = X_test\n",
    "        #X_test_unscal = X_test.drop(['trustLevel'], axis=1)\n",
    "        scaler = prep.StandardScaler()\n",
    "        X_test_scal_np = scaler.fit_transform(X=X_test_unscal) # scaled data as np-array\n",
    "\n",
    "        X_test_cols = X_test_unscal.columns\n",
    "        X_test_scal = pd.DataFrame(X_test_scal_np, columns=X_test_cols)\n",
    "        #X_test_scal_unfinished = pd.DataFrame(X_test_scal_np, columns=X_train_cols)\n",
    "        #X_test_scal= pd.concat([X_test_scal_unfinished, X_test['trustLevel']])\n",
    "\n",
    "\n",
    "\n",
    "        # Models per split   \n",
    "\n",
    "        xgb = XGBClassifier()\n",
    "        xgb.fit(X_train_scal, y_train)\n",
    "        y_pred = xgb.predict(X_test_scal)\n",
    "        score_xgb = profit_scorer(y_test, y_pred)\n",
    "        score_list_xgb.append(score_xgb)\n",
    "\n",
    "        lr = LogisticRegression(C=2)\n",
    "        lr.fit(X_train_scal, y_train)\n",
    "        y_pred = lr.predict(X_test_scal)\n",
    "        score_lr = profit_scorer(y_test,y_pred)\n",
    "        score_list_lr.append(score_lr)\n",
    "\n",
    "        naive_bayes = GaussianNB()\n",
    "        naive_bayes.fit(X_train_scal, y_train)\n",
    "        y_pred = naive_bayes.predict(X_test_scal)\n",
    "        score_gnb = profit_scorer(y_test, y_pred)\n",
    "        score_list_gnb.append(score_gnb)\n",
    "\n",
    "        tree = DecisionTreeClassifier()\n",
    "        tree.fit(X_train_scal, y_train)\n",
    "        y_pred = tree.predict(X_test_scal)\n",
    "        score_tr = profit_scorer(y_test, y_pred)\n",
    "        score_list_tr.append(score_tr)\n",
    "        \n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred = knn.predict(X_test)\n",
    "        score_knn = profit_scorer(y_test, y_pred)\n",
    "        score_list_knn.append(score_knn)  \n",
    "\n",
    "        ranfo = RandomForestClassifier()\n",
    "        ranfo.fit(X_train, y_train)\n",
    "        y_pred = ranfo.predict(X_test)\n",
    "        score_ranfo = profit_scorer(y_test, y_pred)\n",
    "        score_list_ranfo.append(score_ranfo)  \n",
    "\n",
    "        mlp = MLPClassifier()\n",
    "        mlp.fit(X_train, y_train)\n",
    "        y_pred = mlp.predict(X_test)\n",
    "        score_mlp = profit_scorer(y_test, y_pred)\n",
    "        score_list_mlp.append(score_mlp) \n",
    "\n",
    "        svc = SVC()\n",
    "        svc.fit(X_train, y_train)\n",
    "        y_pred = svc.predict(X_test)\n",
    "        score_svc = profit_scorer(y_test, y_pred)\n",
    "        score_list_svc.append(score_svc)\n",
    "\n",
    "\n",
    "    print('upsampled to:', upsam_paras(len(Xy_train[Xy_train[('fraud')]==0])))\n",
    "    print('XGB \\t', sum(score_list_xgb))\n",
    "    print('LR \\t', sum(score_list_lr))\n",
    "    print('GNB \\t', sum(score_list_gnb))\n",
    "    print('TR \\t', sum(score_list_tr))\n",
    "    print('KNN \\t', sum(score_list_knn))\n",
    "    print('RANFO \\t', sum(score_list_ranfo))\n",
    "    print('MLP \\t', sum(score_list_mlp))\n",
    "    print('SVC \\t', sum(score_list_svc))\n",
    "    \n",
    "print('Unsampled number of minority - frauds:', len(Xy_train[Xy_train[('fraud')]==1]))\n",
    "\n",
    "# Preformes surprisingly bad on xgb and lr. Did I do smth wrong? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model until now: XGB without scaling and no upsampling -> 230\n",
    "# and LR with only scaling -> 230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next up: XGB & LR Tuning, MetaCost Algorithm (Lecture 4, ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
