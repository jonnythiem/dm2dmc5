{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import make_scorer\n",
    "from xgboost import XGBClassifier\n",
    "import featuretools as ft\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.utils import resample\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataframe_org = pd.read_csv('train.csv', delimiter='|')\n",
    "dataframe = dataframe_org.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### manual feature generation ##########\n",
    "\n",
    "# totalScanned:\n",
    "dataframe['totalScanned'] = dataframe['scannedLineItemsPerSecond'] * dataframe['totalScanTimeInSeconds']\n",
    "# avgValuePerScan:\n",
    "dataframe['avgTimePerScan'] = 1/ dataframe['scannedLineItemsPerSecond']\n",
    "dataframe['avgValuePerScan'] = dataframe['avgTimePerScan'] * dataframe['valuePerSecond']\n",
    "# manual feature generation - \"totalScanned\" ratios\n",
    "# withoutRegisPerPosition\n",
    "dataframe['withoutRegisPerPosition'] = dataframe['scansWithoutRegistration'] / dataframe['totalScanned']\n",
    "# ratio of scansWithoutRegis in totalScan\n",
    "# equivalent to lineItemVoidsPerPosition\n",
    "# Might indicate how new or ambivalent a customer is. Expected to be higher for low \"trustLevel\"\n",
    "# quantiModPerPosition\n",
    "dataframe['quantiModPerPosition'] = dataframe['quantityModifications'] / dataframe['totalScanned']\n",
    "# ratio of quanityMods in totalScan\n",
    "# manual feature generation - \"grandTotal\" ratios\n",
    "# lineItemVoidsPerTotal\n",
    "dataframe['lineItemVoidsPerTotal'] = dataframe['lineItemVoids'] / dataframe['grandTotal']\n",
    "# withoutRegisPerTotal\n",
    "dataframe['withoutRegisPerTotal'] = dataframe['scansWithoutRegistration'] / dataframe['grandTotal']\n",
    "# quantiModPerTotal\n",
    "dataframe['quantiModPerTotal'] = dataframe['quantityModifications'] / dataframe['grandTotal']\n",
    "# manual feature generation - \"totalScanTimeInSeconds\" ratios\n",
    "# lineItemVoidsPerTime\n",
    "dataframe['lineItemVoidsPerTime'] = dataframe['lineItemVoids'] / dataframe['totalScanTimeInSeconds']\n",
    "# withoutRegisPerTime\n",
    "dataframe['withoutRegisPerTime'] = dataframe['scansWithoutRegistration'] / dataframe['totalScanTimeInSeconds']\n",
    "# quantiModPerTime\n",
    "dataframe['quantiModPerTime'] = dataframe['quantityModifications'] / dataframe['totalScanTimeInSeconds']\n",
    "# extra\n",
    "dataframe['extra'] = dataframe['totalScanned'] * dataframe['totalScanned']/ dataframe['trustLevel']\n",
    "\n",
    "########### end manual feature generation ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the ftarget label fraud at the end of the dataframe\n",
    "\n",
    "dataframe_XG = dataframe[['trustLevel', 'totalScanTimeInSeconds', 'grandTotal', 'lineItemVoids',\n",
    "       'scansWithoutRegistration', 'quantityModifications',\n",
    "       'scannedLineItemsPerSecond', 'valuePerSecond',\n",
    "       'lineItemVoidsPerPosition', 'totalScanned', 'avgTimePerScan',\n",
    "       'avgValuePerScan', 'withoutRegisPerPosition', 'quantiModPerPosition',\n",
    "       'lineItemVoidsPerTotal', 'withoutRegisPerTotal', 'quantiModPerTotal',\n",
    "       'lineItemVoidsPerTime', 'withoutRegisPerTime', 'quantiModPerTime', 'extra', 'fraud']]\n",
    "\n",
    "dataframe_LR = dataframe_XG.drop('extra',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation by Nico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation by Nico\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "def profit_scorer(y, y_pred):\n",
    "    profit_matrix = {(0,0): 0, (0,1): -5, (1,0): -25, (1,1): 5}\n",
    "    return sum(profit_matrix[(pred, actual)] for pred, actual in zip(y_pred, y))\n",
    "profit_scoring = make_scorer(profit_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models with no pre-processing at all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline models of original data w/out pre-processing and default parameters: \n",
      "\n",
      "XGB: -80\n",
      "LR: -360\n",
      "ADB: -130\n"
     ]
    }
   ],
   "source": [
    "#Baseline model of original data w/out pre-processing\n",
    "X_base_org = dataframe_org.drop('fraud',axis=1)\n",
    "y_base_org = dataframe_org['fraud']\n",
    "print('Baseline models of original data w/out pre-processing and default parameters: \\n')\n",
    "print('XGB: {}'.format(sum(cross_validate(XGBClassifier(), X_base_org, y=y_base_org, cv=cv, scoring=profit_scoring)['test_score'])))\n",
    "print('LR: {}'.format(sum(cross_validate(LogisticRegression(), X_base_org, y=y_base_org, cv=cv, scoring=profit_scoring)['test_score'])))\n",
    "print('ADB: {}'.format(sum(cross_validate(AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, random_state=None), X_base_org, y=y_base_org, cv=cv, scoring=profit_scoring)['test_score'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models w/ the manual features generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline models of data w/ manual features generated: \n",
      "\n",
      "XGB: 265\n",
      "LR: 120\n",
      "ADB: 200\n"
     ]
    }
   ],
   "source": [
    "#Baseline models of data w/ manual feature generated\n",
    "X_base = dataframe.drop('fraud',axis=1)\n",
    "y_base = dataframe['fraud']\n",
    "print('Baseline models of data w/ manual features generated: \\n')\n",
    "print('XGB: {}'.format(sum(cross_validate(XGBClassifier(), X_base, y=y_base, cv=cv, scoring=profit_scoring)['test_score'])))\n",
    "print('LR: {}'.format(sum(cross_validate(LogisticRegression(), X_base, y=y_base, cv=cv, scoring=profit_scoring)['test_score'])))\n",
    "print('ADB: {}'.format(sum(cross_validate(AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.5, random_state=None), X_base, y=y_base, cv=cv, scoring=profit_scoring)['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_LR = dataframe_LR.drop('fraud',axis=1)\n",
    "y_LR = dataframe_LR['fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Pipeline: 300\n"
     ]
    }
   ],
   "source": [
    "# create feature union\n",
    "features = []\n",
    "features.append(('select_best', SelectKBest(k=16)))\n",
    "features.append(('pca', PCA(n_components=2)))\n",
    "\n",
    "feature_union = FeatureUnion(features)\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('scaler',sk.preprocessing.StandardScaler()))\n",
    "\n",
    "#Best LR parameters found using exhaustive GridSearch and RandomizedSearch\n",
    "LR = LogisticRegression(C=2, solver='liblinear')\n",
    "\n",
    "estimators.append(('LR',LR))\n",
    "\n",
    "pipe_LR = Pipeline(estimators)\n",
    "print('LR Pipeline: {}'.format(sum(cross_validate(pipe_LR, X_LR, y=y_LR\n",
    "                                                          , cv=cv, scoring=profit_scoring)['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XG Pipeline: 325\n"
     ]
    }
   ],
   "source": [
    "# create feature union\n",
    "features = []\n",
    "features.append(('select_best', SelectKBest(k=11)))\n",
    "features.append(('pca', PCA(n_components=2)))\n",
    "\n",
    "feature_union = FeatureUnion(features)\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('scaler',sk.preprocessing.StandardScaler()))\n",
    "#XGBBoost\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "\n",
    "XG = XGBClassifier(learning_rate=0.1, max_depth=3, min_child_weight=5)\n",
    "estimators.append(('XG',XG))\n",
    "pipe_XG = Pipeline(estimators)\n",
    "\n",
    "print('XG Pipeline: {}'.format(sum(cross_validate(pipe_XG, X_LR, y=y_base\n",
    "                                                          , cv=cv, scoring=profit_scoring)['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADB Pipeline: 225\n"
     ]
    }
   ],
   "source": [
    "# create feature union\n",
    "features = []\n",
    "features.append(('select_best', SelectKBest(k=10)))\n",
    "features.append(('pca', PCA(n_components=2)))\n",
    "\n",
    "feature_union = FeatureUnion(features)\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('scaler',sk.preprocessing.StandardScaler()))\n",
    "\n",
    "#Best LR parameters found using exhaustive GridSearch and RandomizedSearch\n",
    "ADB = AdaBoostClassifier(base_estimator=None, n_estimators=55, learning_rate=0.35)\n",
    "\n",
    "estimators.append(('ADB',ADB))\n",
    "\n",
    "pipe_ADB = Pipeline(estimators)\n",
    "print('ADB Pipeline: {}'.format(sum(cross_validate(pipe_ADB, X_LR, y=y_LR\n",
    "                                                          , cv=cv, scoring=profit_scoring)['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
